{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 2 IMDB Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "dataset=pd.read_csv('data/train_dataset.csv')\n",
    "dataset.drop(columns=['id'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview Data\n",
    "dataset.shape\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seperate dataset into features and classes\n",
    "y = dataset['imdb_score_binned']\n",
    "dataset.drop(columns=['imdb_score_binned'], inplace=True)\n",
    "X = dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will find the mean imdb score\n",
    "mean_imdb_score = y.mean()\n",
    "print(mean_imdb_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making a histogram of count of imdb score\n",
    "plt.rcParams.update({'font.size': 22})\n",
    "plt.figure(figsize=(12,8))\n",
    "print(y.name)\n",
    "\n",
    "ax = sns.countplot(x=y.name, data=y.to_frame())\n",
    "plt.title('Count of Movie Ratings')\n",
    "plt.xlabel('IMDB Rating Binned')\n",
    "plt.ylabel('Count')\n",
    "total = y.count()\n",
    "ax.bar_label(ax.containers[0], fmt=lambda x: f'{(x/total)*100:0.1f}%')\n",
    "ax.margins(y=0.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will standardise some of the features. We will use z-score standardisation as it is better for zero mean models.\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all numeric attributes\n",
    "numeric = X.select_dtypes(include='number')\n",
    "numeric.drop(columns=['title_year'], inplace=True)\n",
    "numeric.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 12})\n",
    "# Check distributions before normalisation\n",
    "for c in numeric.columns:\n",
    "    # Looking at the distribution of the duration of the movies\n",
    "    plt.figure(figsize=(10,6))\n",
    "    sns.histplot(X[c])\n",
    "    plt.title(f'Distribution of {c}')\n",
    "    plt.xlabel(c)\n",
    "    plt.ylabel('Number of Movies')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalise the fields\n",
    "for c in numeric.columns:\n",
    "    X[c] = scaler.fit_transform(X[[c]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check fields are normalised\n",
    "for c in numeric.columns:\n",
    "    # Looking at the distribution of the duration of the movies\n",
    "    plt.figure(figsize=(10,6))\n",
    "    sns.histplot(X[c])\n",
    "    plt.title(f'Distribution of {c}')\n",
    "    plt.xlabel(c)\n",
    "    plt.ylabel('Number of Movies')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will do a correlation table between all the columns.\n",
    "print(y.to_frame())\n",
    "data = pd.concat([X.select_dtypes(include='number'), y.to_frame()], axis=1)\n",
    "correlation = (data).corr()\n",
    "# We will plot the correlation table\n",
    "plt.figure(figsize=(12,8))\n",
    "sns.heatmap(correlation, annot=True, cmap='coolwarm')\n",
    "plt.title('Correlation Table')\n",
    "plt.show()\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing Highly Correlated Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cast_total_facebook_likes are correlated heavily with actor_1_facebook_likes, actor_2_facebook_likes, and actor_3_facebook_likes, so we drop the actor facebook likes\n",
    "X.drop(columns=['actor_1_facebook_likes', 'actor_2_facebook_likes', 'actor_3_facebook_likes'], inplace=True)\n",
    "\n",
    "# num_voted_users and num_user_for_reviews are highly correlated, so we drop the number of user reviews\n",
    "X.drop(columns=['num_user_for_reviews'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing Text Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will drop the id, director name, actor names, movie title columns, and plot keywords.\n",
    "X.drop(columns=['director_name', 'actor_1_name', 'actor_2_name', 'actor_3_name', 'movie_title', 'plot_keywords'], inplace=True)\n",
    "\n",
    "# Removing title embedding\n",
    "X.drop(columns=['title_embedding'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing Attributes without much variation\n",
    "These attributes have a lot of unique values, with one value being predominant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" for c in X.columns:\n",
    "    plt.figure(figsize=(30,8))\n",
    "    sns.countplot(x=c, data=X)\n",
    "    plt.title(f'Count of Movies {c}')\n",
    "    plt.xlabel(c)\n",
    "    plt.ylabel('Count')\n",
    "    plt.show() \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.drop(columns=['country', 'language', 'content_rating'], inplace=True)\n",
    "X.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get doc2vec data for plot.\n",
    "\n",
    "This is for vectorizing plot, basically a better bagofwords. If we want to use the movie's plot as a feature instead of useing words we should use this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2vec_plot_data = np.load('data/features_doc2vec/train_doc2vec_features_plot_keywords.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are assuming that if we have a movie that has a higly related plot point to a film in training dataset then the movie will have a similar score.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "doc2vec_scaled = scaler.fit_transform(doc2vec_plot_data)\n",
    "# Now we create a similarity matrix using cosine similarity\n",
    "similarity_matrix = cosine_similarity(doc2vec_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will now create a matrix that is a matrix of difference of rating between movies\n",
    "imdb_score_matrix = np.array(y)\n",
    "imdb_score_matrix = imdb_score_matrix.reshape(-1,1)\n",
    "imdb_score_matrix = np.repeat(imdb_score_matrix, len(imdb_score_matrix), axis=1)\n",
    "imdb_score_matrix = np.abs(imdb_score_matrix - imdb_score_matrix.T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we will scatter plot cosine similarity and imdb score difference\n",
    "plt.figure(figsize=(12,8))\n",
    "sns.scatterplot(x=similarity_matrix.flatten(), y=imdb_score_matrix.flatten())\n",
    "plt.title('Cosine Similarity vs IMDB Score Difference')\n",
    "plt.xlabel('Cosine Similarity')\n",
    "plt.ylabel('IMDB Score Difference')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Hot Encoding Genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have a genre column which has multiple genres separated by '|'. We will split these genres into separate columns.\n",
    "# We will first find all the unique genres\n",
    "unique_genres = set()\n",
    "for genre in X['genres']:\n",
    "    genre_list = genre.split('|')\n",
    "    for g in genre_list:\n",
    "        unique_genres.add(g)\n",
    "unique_genres = list(unique_genres)\n",
    "print(unique_genres)\n",
    "# Now we split the genres into separate columns\n",
    "for genre in unique_genres:\n",
    "    X[genre] = X['genres'].str.contains(genre).astype(int)\n",
    "X.drop(columns=['genres'], inplace=True)\n",
    "X.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will also store the genres in a separate variable because we want to do stepwise selection for feature selection\n",
    "genres = X[unique_genres]\n",
    "X.drop(columns=unique_genres, inplace=True)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Assuming X and y are defined elsewhere in your code:\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_genres = pd.concat([X, genres], axis=1)\n",
    "# Adding genres to training set\n",
    "X_train_genres = pd.concat([X_train, genres.iloc[X_train.index]], axis=1)\n",
    "# Adding genres to test set\n",
    "X_test_genres = pd.concat([X_test, genres.iloc[X_test.index]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, estimator, param_grid):\n",
    "        self.estimator = estimator\n",
    "        self.param_grid = param_grid\n",
    "\n",
    "    def grid_search(self, cv=5, n_jobs=-1, verbose=False, refit=True):\n",
    "        self.gs = GridSearchCV(self.estimator, param_grid=self.param_grid, cv=cv, n_jobs=n_jobs, verbose=verbose, refit=refit)\n",
    "    \n",
    "    def get_best_model(self, X_train, y_train):\n",
    "        self.gs.fit(X_train, y_train)\n",
    "        self.best_estimator = self.gs.best_estimator_\n",
    "        self.best_params = self.gs.best_params_\n",
    "        self.best_cv = self.gs.best_score_\n",
    "        return self.best_estimator, self.best_params, self.best_cv \n",
    "\n",
    "    def predict(self, X_test):\n",
    "        y_pred = self.best_estimator.predict(X_test)\n",
    "        return y_pred\n",
    "    \n",
    "    def sfs(self):\n",
    "        self.sffs = SFS(self.best_estimator, k_features='best', forward=True, floating=False, cv=None, verbose = False)\n",
    "    \n",
    "    def get_best_features(self, X_train, y_train):\n",
    "        self.sffs = self.sffs.fit(X_train, y_train)\n",
    "        best_subset = self.sffs.subsets_[max(self.sffs.subsets_, key=lambda k: self.sffs.subsets_[k]['avg_score'])]\n",
    "        return list(best_subset['feature_names'])\n",
    "\n",
    "    def fit_with_features(self, X_train, y_train, features=None):\n",
    "        if features is None:\n",
    "            self.best_estimator.fit(X_train, y_train)\n",
    "        else: \n",
    "            self.best_estimator.fit(X_train[features], y_train)\n",
    "\n",
    "    def get_learning_curve(self, X_train, y_train):\n",
    "        self.train_sizes, self.train_scores, self.test_scores = learning_curve(self.best_estimator, X_train, y_train, cv=5)\n",
    "\n",
    "        train_mean = np.mean(self.train_scores, axis=1)\n",
    "        train_std = np.std(self.train_scores, axis=1)\n",
    "        test_mean = np.mean(self.test_scores, axis=1)\n",
    "        test_std = np.std(self.test_scores, axis=1)\n",
    "\n",
    "        # Plot learning curves\n",
    "        plt.plot(self.train_sizes, train_mean, label=\"Training score\")\n",
    "        plt.plot(self.train_sizes, test_mean, label=\"Cross-validation score\")\n",
    "        plt.fill_between(self.train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.1)\n",
    "        plt.fill_between(self.train_sizes, test_mean - test_std, test_mean + test_std, alpha=0.1)\n",
    "        plt.xlabel(\"Training Set Size\")\n",
    "        plt.ylabel(\"Accuracy Score\")\n",
    "        plt.title(\"Learning Curves\")\n",
    "        plt.legend()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1: Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM Model\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10],  # Regularization parameter\n",
    "    'gamma': [0.1, 0.01, 0.001],  # Kernel coefficient for RBF, polynomial and sigmoid\n",
    "    'kernel': ['poly', 'linear', 'rbf']  # Type of kernel\n",
    "}\n",
    "\n",
    "svm = Model(SVC(class_weight = 'balanced', max_iter=10000), param_grid)\n",
    "svm.grid_search()\n",
    "svm_best, svm_params, svm_cv = svm.get_best_model(X_train_genres, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing parameters for the best estimator including kernel)\n",
    "print(svm_params)\n",
    "y_pred_svm = svm.predict(X_test_genres)\n",
    "svm_accuracy = accuracy_score(y_test, y_pred_svm)\n",
    "print(f'SVM Accuracy: {svm_accuracy}')\n",
    "print(f'SVM Cross Validation Score Best: {svm_cv}')\n",
    "svm_cv_scores = cross_val_score(svm_best, X, y, cv=5)\n",
    "print(f'SVM Cross Validation Score Average: {svm_cv_scores.mean()}')\n",
    "print(classification_report(y_test, y_pred_svm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm.get_learning_curve(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doing feature selection with the best SVM model\n",
    "svm.sfs()\n",
    "svm_best_features = svm.get_best_features(X_train, y_train)\n",
    "print(svm_best_features)\n",
    "svm_best_features += unique_genres\n",
    "svm.fit_with_features(X_train_genres, y_train, svm_best_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now checking accuracy with the best features\n",
    "y_pred_svm_ft = svm.predict(X_test_genres[svm_best_features])\n",
    "svm_accuracy_ft = accuracy_score(y_test, y_pred_svm_ft)\n",
    "svm_cv_scores = cross_val_score(svm_best, X_genres, y, cv=5)\n",
    "print(f'SVM Accuracy with selected features: {svm_accuracy_ft}')\n",
    "print(f'SVM Cross Validation Score with Best Features: {svm_cv_scores.mean()}')\n",
    "print(classification_report(y_test, y_pred_svm_ft))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm.get_learning_curve(X_genres[svm_best_features], y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2: Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression Model\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100],  # Regularization strength\n",
    "    'penalty': ['l1', 'l2', 'elasticnet', 'none'],  # Types of penalties\n",
    "    'solver': ['saga'],  # Solvers\n",
    "    'max_iter': [100,200,1000]  # Number of iterations\n",
    "}\n",
    "logr = Model(LogisticRegression(), param_grid)\n",
    "logr.grid_search()\n",
    "logr_best, logr_params, logr_cv = logr.get_best_model(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing parameters for the best estimator including kernel)\n",
    "print(logr_params)\n",
    "y_pred_logr = logr.predict(X_test)\n",
    "logr_accuracy = accuracy_score(y_test, y_pred_logr)\n",
    "print(f'Logistic Regression Accuracy: {logr_accuracy}')\n",
    "print(f'Logistic Regression Cross Validation Score Best: {logr_cv}')\n",
    "logr_cv_scores = cross_val_score(logr_best, X, y, cv=5)\n",
    "print(f'Logistic Regression Cross Validation Score Average: {logr_cv_scores.mean()}')\n",
    "print(classification_report(y_test, y_pred_logr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logr.get_learning_curve(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doing feature selection with the best logr model\n",
    "logr.sfs()\n",
    "logr_best_features = logr.get_best_features(X_train, y_train)\n",
    "print(logr_best_features)\n",
    "logr_best_features += unique_genres\n",
    "logr.fit_with_features(X_train_genres, y_train, logr_best_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now checking accuracy with the best features\n",
    "y_pred_logr_ft = logr.predict(X_test_genres[logr_best_features])\n",
    "logr_accuracy_ft = accuracy_score(y_test, y_pred_logr_ft)\n",
    "logr_cv_scores = cross_val_score(logr_best, X_genres, y, cv=5)\n",
    "print(f'Logistic Regression Accuracy with selected features: {logr_accuracy_ft}')\n",
    "print(f'Logistic Regression Cross Validation Score with Best Features: {logr_cv_scores.mean()}')\n",
    "print(classification_report(y_test, y_pred_logr_ft))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logr.get_learning_curve(X_genres[logr_best_features], y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3: Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Model\n",
    "param_grid = {'n_estimators': [50, 100, 200], 'max_depth': [10, 20, 30], 'max_features': ['auto', 'sqrt', 'log2']}\n",
    "rf = Model(RandomForestClassifier(random_state=42), param_grid)\n",
    "rf.grid_search()\n",
    "rf_best, rf_params, rf_cv = rf.get_best_model(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing parameters for the best estimator including kernel)\n",
    "print(rf_params)\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "rf_accuracy = accuracy_score(y_test, y_pred_rf)\n",
    "print(f'Random Forest Accuracy: {rf_accuracy}')\n",
    "print(f'Random Forest Cross Validation Score Best: {rf_cv}')\n",
    "rf_cv_scores = cross_val_score(rf_best, X, y, cv=5)\n",
    "print(f'Random Forest Cross Validation Score Average: {rf_cv_scores.mean()}')\n",
    "print(classification_report(y_test, y_pred_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting learning curve for random forest\n",
    "rf.get_learning_curve(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doing feature selection with the best random forest model\n",
    "rf.sfs()\n",
    "rf_best_features = rf.get_best_features(X_train, y_train)\n",
    "print(rf_best_features)\n",
    "rf_best_features += unique_genres\n",
    "rf.fit_with_features(X_train_genres, y_train, rf_best_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now checking accuracy with the best features\n",
    "y_pred_rf_ft = rf.predict(X_test_genres[rf_best_features])\n",
    "rf_accuracy_ft = accuracy_score(y_test, y_pred_rf_ft)\n",
    "rf_cv_scores = cross_val_score(rf_best, X, y, cv=5)\n",
    "print(f'Random Forest Accuracy with selected features: {rf_accuracy_ft}')\n",
    "print(f'Random Forest Cross Validation Score with Best Features: {rf_cv_scores.mean()}')\n",
    "print(classification_report(y_test, y_pred_rf_ft))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting learning curve for random forest with best features\n",
    "rf.get_learning_curve(X_genres[rf_best_features], y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 4: Ensemble Model by Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import StackingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimators = [('rf', rf_best), ('logr', logr_best)]\n",
    "stack = StackingClassifier(estimators=estimators, final_estimator=logr_best)\n",
    "stack.fit(X_train_genres, y_train)\n",
    "y_pred_stack = stack.predict(X_test_genres)\n",
    "stack_cv_scores = cross_val_score(stack, X_genres, y, cv=5)\n",
    "print(f'Stacking Classifier Cross Validation Score: {stack_cv_scores.mean()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting learning curve for stacking\n",
    "train_sizes, train_scores, test_scores = learning_curve(stack, X_genres, y, cv=5)\n",
    "\n",
    "train_mean = np.mean(train_scores, axis=1)\n",
    "train_std = np.std(train_scores, axis=1)\n",
    "test_mean = np.mean(test_scores, axis=1)\n",
    "test_std = np.std(test_scores, axis=1)\n",
    "\n",
    "# Plot learning curves\n",
    "plt.plot(train_sizes, train_mean, label=\"Training score\")\n",
    "plt.plot(train_sizes, test_mean, label=\"Cross-validation score\")\n",
    "plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.1)\n",
    "plt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, alpha=0.1)\n",
    "plt.xlabel(\"Training Set Size\")\n",
    "plt.ylabel(\"Accuracy Score\")\n",
    "plt.title(\"Learning Curves\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction on test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading test dataset\n",
    "test_dataset = pd.read_csv('data/test_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all numeric attributes\n",
    "numeric = test_dataset.select_dtypes(include='number')\n",
    "numeric.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalise the fields\n",
    "for c in numeric.columns:\n",
    "    test_dataset[c] = scaler.fit_transform(test_dataset[[c]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping columns to match training dataset\n",
    "test_dataset.drop(columns=['actor_1_facebook_likes', 'actor_2_facebook_likes', 'actor_3_facebook_likes'], inplace=True)\n",
    "test_dataset.drop(columns=['num_user_for_reviews'], inplace=True)\n",
    "test_dataset.drop(columns=['director_name', 'actor_1_name', 'actor_2_name', 'actor_3_name', 'movie_title', 'plot_keywords'], inplace=True)\n",
    "test_dataset.drop(columns=['title_embedding'], inplace=True)\n",
    "test_dataset.drop(columns=['country', 'language', 'content_rating'], inplace=True)\n",
    "test_dataset.drop(columns=['id'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting genres into separate columns\n",
    "for genre in unique_genres:\n",
    "    test_dataset[genre] = test_dataset['genres'].str.contains(genre).astype(int)\n",
    "test_dataset.drop(columns=['genres'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions for each model\n",
    "# svm\n",
    "y_pred_svm = svm.predict(test_dataset[svm_best_features])\n",
    "test_dataset['imdb_score_binned'] = y_pred_svm\n",
    "test_dataset['id'] = test_dataset.index +1\n",
    "test_dataset[['id', 'imdb_score_binned']].to_csv('svm_predictions.csv', index=False)\n",
    "test_dataset.drop(columns=['imdb_score_binned', 'id'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# logr\n",
    "y_pred_logr = logr.predict(test_dataset[logr_best_features])\n",
    "test_dataset['imdb_score_binned'] = y_pred_logr\n",
    "test_dataset['id'] = test_dataset.index+1\n",
    "test_dataset[['id', 'imdb_score_binned']].to_csv('logr_predictions.csv', index=False)\n",
    "test_dataset.drop(columns=['imdb_score_binned', 'id'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# rf\n",
    "y_pred_rf = rf.predict(test_dataset[rf_best_features])\n",
    "test_dataset['imdb_score_binned'] = y_pred_rf\n",
    "test_dataset['id'] = test_dataset.index +1\n",
    "test_dataset[['id', 'imdb_score_binned']].to_csv('rf_predictions.csv', index=False)\n",
    "test_dataset.drop(columns=['imdb_score_binned', 'id'], inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stack\n",
    "y_pred_stack = stack.predict(test_dataset)\n",
    "test_dataset['imdb_score_binned'] = y_pred_stack\n",
    "test_dataset['id'] = test_dataset.index +1\n",
    "test_dataset[['id', 'imdb_score_binned']].to_csv('stack_predictions.csv', index=False)\n",
    "test_dataset.drop(columns=['imdb_score_binned', 'id'], inplace=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
